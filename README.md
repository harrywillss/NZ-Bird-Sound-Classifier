# ğŸ¦ New Zealand Bird Sound Classifier

*A machine learning project for classifying New Zealand native bird species using audio recordings and Vision Transformer (ViT) models.*

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ“– Overview

This project implements a novel approach to bird species classification by treating audio spectrograms as visual data and using Vision Transformer (ViT) models for classification. The system can identify 10+ species of New Zealand native birds from their calls and songs.

### Key Features

- ğŸµ **Audio-to-Vision Approach**: Converts bird audio recordings to mel spectrograms for visual classification
- ğŸ¤– **Vision Transformer Model**: Uses ViT/ViTHybrid architecture for state-of-the-art classification performance
- ğŸ”§ **LoRA Fine-tuning**: Efficient parameter-efficient fine-tuning approach
- ğŸŒ¿ **Native Species Focus**: Specializes in New Zealand endemic and native bird species
- ğŸ“Š **Comprehensive Dataset**: 555+ audio recordings across 10 primary species

## ğŸ¦œ Supported Bird Species

The model currently classifies the following New Zealand bird species:

| Species | Count | Scientific Name |
|---------|-------|----------------|
| TÅ«Ä« | 167 | *Prosthemadera novaeseelandiae* |
| New Zealand Bellbird | 77 | *Anthornis melanura* |
| Whitehead | 44 | *Mohoua albicilla* |
| New Zealand Fantail | 43 | *Rhipidura fuliginosa* |
| Robin | 43 | *Petroica longipes* |
| KÄkÄ | 41 | *Nestor meridionalis* |
| Saddleback | 39 | *Philesturnus rufusater* |
| Tomtit | 39 | *Petroica macrocephala* |
| Morepork | 31 | *Ninox novaeseelandiae* |
| Silvereye | 30 | *Zosterops lateralis* |

## ğŸ—ï¸ Project Structure

```text
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ brainstorm.md            # Project planning and research notes
â”œâ”€â”€ data_downloader.py       # Xeno-canto API data acquisition script
â”œâ”€â”€ count_specie.py          # Species count analysis
â”œâ”€â”€ sample_rate.py           # Audio preprocessing and resampling
â”œâ”€â”€ finetuning.ipynb         # Model training notebook
â”œâ”€â”€ recordings_data.csv      # Dataset metadata
â”œâ”€â”€ recordings_metadata.json # Detailed recording information
â”œâ”€â”€ label_counts.txt         # Species distribution summary
â”œâ”€â”€ downloads/               # Raw audio files (.wav)
â”œâ”€â”€ resampled/              # Preprocessed audio files
â””â”€â”€ reports/                # Project documentation and reports
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- PyTorch
- Transformers (Hugging Face)
- librosa
- pandas
- requests
- tqdm
- soundfile

### Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/harrywillss/Tio.git
   cd Tio
   ```

2. **Install dependencies**

   ```bash
   pip install torch torchvision transformers librosa pandas requests tqdm soundfile python-dotenv
   ```

3. **Set up environment variables**

   Create a `.env` file in the project root:

   ```env
   XC_API_KEY=your_xeno_canto_api_key
   ```

### Usage

1. **Download bird recording data**

   ```bash
   python data_downloader.py
   ```

2. **Preprocess audio files**

   ```bash
   python sample_rate.py
   ```

3. **Analyze species distribution**

   ```bash
   python count_specie.py
   ```

4. **Train the model**

   Open and run `finetuning.ipynb` in Jupyter Notebook or your preferred environment.

## ğŸ”¬ Methodology

### Data Collection

- **Source**: Xeno-canto bird sound database
- **Quality Filter**: Grade C or better recordings
- **Geographic Filter**: New Zealand recordings only
- **Type Filter**: Calls and songs
- **Total Dataset**: 555+ recordings from 10+ species

### Preprocessing Pipeline

1. **Audio Standardization**: Resample all recordings to 44.1kHz
2. **Mel Spectrogram Conversion**: Transform audio to visual representations
3. **Data Augmentation**: Potential audio segmentation for dataset expansion
4. **Quality Control**: Filter segments with insufficient data

### Model Architecture

- **Base Model**: Vision Transformer (ViT) or ViT Hybrid
- **Fine-tuning Strategy**: LoRA (Low-Rank Adaptation) for efficient training
- **Input Format**: Mel spectrograms treated as images
- **Output**: Multi-class classification (10+ bird species)

## ğŸ“Š Dataset Statistics

- **Total Recordings**: 555 audio files
- **Average Length**: ~80.61 seconds
- **Audio Format**: WAV files at 44.1kHz
- **Species Coverage**: 10 primary species (>30 recordings each)
- **Data Quality**: Xeno-canto grade C or better

## ğŸ¯ Project Roadmap

- [x] Data collection from Xeno-canto API
- [x] Audio preprocessing and standardization
- [x] Species analysis and filtering
- [ ] Mel spectrogram generation
- [ ] ViT model implementation
- [ ] LoRA fine-tuning setup
- [ ] Model training and validation
- [ ] Performance evaluation
- [ ] Real-time classification application
- [ ] Web interface for audio upload and classification

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues, fork the repository, and create pull requests.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Xeno-canto**: For providing the comprehensive bird sound database
- **Hugging Face**: For the Transformers library and pre-trained models
- **New Zealand Department of Conservation**: For species information and conservation efforts
- **AIML339 Course**: Academic framework and guidance

## ğŸ“§ Contact

For questions or collaboration opportunities, please reach out through the GitHub repository issues.

---

*This project is part of AIML339 coursework focusing on innovative applications of machine learning in bioacoustics and conservation.*
